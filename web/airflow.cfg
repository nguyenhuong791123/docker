[core]
logging_level = info
airflow_home = /airflow
dags_folder = /airflow/dags
remote_logging = True
remote_log_conn_id = airflow_logs
#remote_log_conn_id = s3://${AWS_ACCESS_KEY_ID}:${AWS_SECRET_ACCESS_KEY}@${AWS_S3_BUCKET_NAME}/airflow
remote_base_log_folder = s3://${AWS_S3_BUCKET_NAME}/af-worker
encrypt_s3_logs = False
logging_config_class = log_config.LOGGING_CONFIG
task_log_reader = s3.task
worker_log_server_port = 8793
executor = CeleryExecutor
sql_alchemy_conn = mysql://root:${MYSQL_ENV_MYSQL_ROOT_PASSWORD}@${MYSQL_PORT_3306_TCP_ADDR}:3306/airflow
parallelism = 32
dag_concurrency = 16
load_examples = False
max_active_runs_per_dag = 1
plugins_folder = /airflow/plugins
task_runner = BashTaskRunner
fernet_key = qo4h8QRyjXAXIrrGicXJixewan-nZ_MKEb2ON_GTpis=

[webserver]
base_url = http://localhost:8080
web_server_host = 0.0.0.0
web_server_port = 8080

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_owner = Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = nguyenhuong791123@gmail.com
smtp_port = 587
smtp_password = huong080
smtp_mail_from = nguyenhuong791123@gmail.com

[celery]
celery_app_name = airflow.executors.celery_executor
celeryd_concurrency = 16
worker_log_server_port = 8793
broker_url = redis://${REDIS_PORT_6379_TCP_ADDR}:6379/1
celery_result_backend = db+mysql://root:${MYSQL_ENV_MYSQL_ROOT_PASSWORD}@${MYSQL_PORT_3306_TCP_ADDR}:3306/airflow
flower_port = 5555
default_queue = default

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
catchup_by_default = False
